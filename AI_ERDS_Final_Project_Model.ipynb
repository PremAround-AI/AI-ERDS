{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNG2MOzOExqvt/CUMuCyjeO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PremAround-AI/AI-ERDS/blob/main/AI_ERDS_Final_Project_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "lNqQMjCpgmr1",
        "outputId": "996ffda2-e23d-48e2-a7c5-e949c2616135"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cv2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3971759925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BZJtMm9grn2",
        "outputId": "215db810-eeda-4263-eec4-b729e55f9bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This code already made sequence folder in different file, so doesn't run the file here.\n",
        "\n",
        "# Configuration\n",
        "video_base_dir = \"/content/drive/MyDrive/Project_DataSet\" # Base directory containing class folders\n",
        "output_dir = \"drive/MyDrive/sequences\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "SEQ_LEN = 15\n",
        "IMG_SIZE = 64\n",
        "STRIDE = 2\n",
        "AUGMENTATIONS_PER_CLIP = 2\n",
        "\n",
        "# Augmentation pipeline\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Rotate(limit=10, p=0.3)\n",
        "])\n",
        "\n",
        "def augment_clip(clip):\n",
        "    \"\"\"Applies defined augmentations to a list of frames.\"\"\"\n",
        "    # Albumentations expects a single image, so we apply it frame by frame\n",
        "    return [transform(image=frame)[\"image\"] for frame in clip]\n",
        "\n",
        "print(\"Starting video processing and augmentation...\")\n",
        "\n",
        "# Check if the base directory exists\n",
        "if not os.path.isdir(video_base_dir):\n",
        "    print(f\"Error: Directory '{video_base_dir}' not found.\")\n",
        "    print(\"Please verify the path to your dataset in Google Drive.\")\n",
        "else:\n",
        "    # Get all class folders and sort them to ensure consistent selection\n",
        "    all_class_folders = sorted([d for d in os.listdir(video_base_dir) if os.path.isdir(os.path.join(video_base_dir, d))])\n",
        "\n",
        "    # Use only the first 5 folders (classes)\n",
        "    class_folders_to_process = all_class_folders[:5]\n",
        "\n",
        "    print(f\"Processing a total of {len(class_folders_to_process)} classes.\")\n",
        "\n",
        "    for label_index, label in enumerate(class_folders_to_process):\n",
        "        class_path = os.path.join(video_base_dir, label)\n",
        "        print(f\"\\nProcessing class {label_index + 1}/{len(class_folders_to_process)}: {label}\")\n",
        "\n",
        "        # Get all video files in the current class folder and sort them\n",
        "        all_video_files = sorted([f for f in os.listdir(class_path) if f.endswith(\".mp4\") or f.endswith(\".avi\")]) # Added .avi just in case\n",
        "\n",
        "        # Limit to first 40 videos per class (if available)\n",
        "        video_files_to_process = all_video_files[:40]\n",
        "\n",
        "        if not video_files_to_process:\n",
        "            print(f\"  No video files found in class: {label}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  Processing {len(video_files_to_process)} videos from class {label}.\")\n",
        "\n",
        "        for video_index, video_file in enumerate(video_files_to_process):\n",
        "            video_path = os.path.join(class_path, video_file)\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "            if not cap.isOpened():\n",
        "                print(f\"  Error: Could not open video file {video_path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            frames = []\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                # Resize frame to IMG_SIZE\n",
        "                frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "                frames.append(frame)\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            if not frames:\n",
        "                print(f\"  Warning: No frames read from video {video_file}. Skipping clip generation for this video.\")\n",
        "                continue\n",
        "\n",
        "            # Generate clips and save\n",
        "            clip_count = 0\n",
        "            for j in range(0, len(frames) - SEQ_LEN + 1, STRIDE):\n",
        "                clip = frames[j:j + SEQ_LEN]\n",
        "\n",
        "                # Ensure the clip has the required sequence length\n",
        "                if len(clip) != SEQ_LEN:\n",
        "                    continue # Skip if the last clip is shorter than SEQ_LEN\n",
        "\n",
        "                # Save original clip\n",
        "                # The naming convention will be {class_name}_{video_index_in_class}_{clip_start_frame_index}.npy\n",
        "                clip_filename = f\"{label}_{video_index:03d}_{j:03d}.npy\"\n",
        "                np.save(os.path.join(output_dir, clip_filename), np.array(clip))\n",
        "                clip_count += 1\n",
        "\n",
        "                # Augment clip and save augmented versions\n",
        "                for k in range(AUGMENTATIONS_PER_CLIP):\n",
        "                    aug_clip = augment_clip(clip)\n",
        "                    aug_clip_filename = f\"{label}_{video_index:03d}_{j:03d}_aug{k}.npy\"\n",
        "                    np.save(os.path.join(output_dir, aug_clip_filename), np.array(aug_clip))\n",
        "\n",
        "            print(f\"    Processed video {video_index + 1}/{len(video_files_to_process)} ({video_file}) - Generated {clip_count} clips.\")\n",
        "\n",
        "    print(\"\\nAll specified sequences and augmentations saved to 'sequences/' directory.\")"
      ],
      "metadata": {
        "id": "mQ4S_w33itaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2Y7doTahVUH",
        "outputId": "b09e532b-9db4-4528-f358-2cce7ef7646b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n",
            "2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_metadata.py\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "SEQUENCE_DIR = \"drive/MyDrive/sequences\"\n",
        "SEQ_LEN = 15\n",
        "IMG_SIZE = 64\n",
        "metadata = []\n",
        "\n",
        "print(\"Generating metadata cache...\")\n",
        "\n",
        "for file in os.listdir(SEQUENCE_DIR):\n",
        "    if file.endswith(\".npy\"):\n",
        "        parts = file.split(\"_\")\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        label = parts[0]\n",
        "        path = os.path.join(SEQUENCE_DIR, file)\n",
        "        try:\n",
        "            shape = np.load(path, mmap_mode='r').shape\n",
        "            if shape == (SEQ_LEN, IMG_SIZE, IMG_SIZE, 3):\n",
        "                metadata.append({\"file\": path, \"label\": label})\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "with open(\"sequence_metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "print(f\"Saved metadata for {len(metadata)} valid files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAz958vKm1mB",
        "outputId": "ce12b91c-087d-4025-c4fa-69aecbeb005d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating metadata cache...\n",
            "Saved metadata for 26886 valid files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import TimeDistributed, GlobalAveragePooling2D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "# Configuration\n",
        "SEQUENCE_DIR = \"drive/MyDrive/sequences\"\n",
        "SEQ_LEN = 15\n",
        "IMG_SIZE = 64\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "MODEL_SAVE_PATH = \"sign_model.h5\"\n",
        "LABEL_MAP_PATH = \"label_map.json\"\n",
        "METADATA_PATH = \"sequence_metadata.json\"\n",
        "\n",
        "# --- Step 1: Load metadata ---\n",
        "print(\"Loading metadata from cache...\")\n",
        "with open(METADATA_PATH, \"r\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "X_files = [entry[\"file\"] for entry in metadata]\n",
        "y_labels = [entry[\"label\"] for entry in metadata]\n",
        "\n",
        "print(f\"Found {len(X_files)} valid sequence files.\")\n",
        "\n",
        "# --- Step 2: Encode labels ---\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_labels)\n",
        "\n",
        "label_map = {str(idx): str(cls) for idx, cls in enumerate(le.classes_)}\n",
        "with open(LABEL_MAP_PATH, \"w\") as f:\n",
        "    json.dump(label_map, f, indent=4)\n",
        "print(f\"Label map saved: {label_map}\")\n",
        "\n",
        "# --- Step 3: Train-test split ---\n",
        "min_samples_per_class = min(np.bincount(y_encoded))\n",
        "if min_samples_per_class < 2:\n",
        "    print(\"Not enough samples per class for stratified split. Using basic split.\")\n",
        "    X_train_files, X_test_files, y_train, y_test = train_test_split(\n",
        "        X_files, y_encoded, test_size=0.2, random_state=42)\n",
        "else:\n",
        "    X_train_files, X_test_files, y_train, y_test = train_test_split(\n",
        "        X_files, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "    print(\"Stratified train-test split applied.\")\n",
        "\n",
        "print(f\"Train: {len(X_train_files)} | Test: {len(X_test_files)}\")\n",
        "\n",
        "# --- Step 4: Data generator ---\n",
        "class SequenceDataGenerator(Sequence):\n",
        "    def __init__(self, file_paths, labels, batch_size, shuffle=True):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(self.file_paths))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.file_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_files = [self.file_paths[i] for i in batch_indices]\n",
        "        batch_labels = [self.labels[i] for i in batch_indices]\n",
        "\n",
        "        X = np.array([np.load(f).astype('float32') / 255.0 for f in batch_files])\n",
        "        y = np.array(batch_labels)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "train_gen = SequenceDataGenerator(X_train_files, y_train, BATCH_SIZE)\n",
        "val_gen = SequenceDataGenerator(X_test_files, y_test, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Train batches per epoch: {len(train_gen)}\")\n",
        "print(f\"Validation batches per epoch: {len(val_gen)}\")\n",
        "\n",
        "# --- Step 5: Build the model ---\n",
        "def build_model(seq_len, img_size, num_classes):\n",
        "    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        TimeDistributed(base_model, input_shape=(seq_len, img_size, img_size, 3)),\n",
        "        TimeDistributed(GlobalAveragePooling2D()),\n",
        "        LSTM(128, return_sequences=False),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "num_classes = len(le.classes_)\n",
        "model = build_model(SEQ_LEN, IMG_SIZE, num_classes)\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# --- Step 6: Train the model ---\n",
        "print(\"Training the model...\")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    MODEL_SAVE_PATH, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', patience=10, min_delta=1e-4, verbose=1, mode='min', restore_best_weights=True\n",
        ")\n",
        "\n",
        "callbacks_list = [checkpoint, early_stopping]\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# --- Step 7: Evaluate the model ---\n",
        "print(\"Evaluating on the test set...\")\n",
        "best_model = load_model(MODEL_SAVE_PATH)\n",
        "loss, acc = best_model.evaluate(val_gen, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CUpacSLG04I-",
        "outputId": "fc908ad9-eac2-4c36-8a6c-950b05c596b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading metadata from cache...\n",
            "Found 26886 valid sequence files.\n",
            "Label map saved: {'0': 'fire', '1': 'road', '2': 'voilence'}\n",
            "Stratified train-test split applied.\n",
            "Train: 21508 | Test: 5378\n",
            "Train batches per epoch: 2689\n",
            "Validation batches per epoch: 673\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2-154406971.py:90: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size, 3))\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">721,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m721,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,987,843</span> (11.40 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,987,843\u001b[0m (11.40 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">729,859</span> (2.78 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m729,859\u001b[0m (2.78 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to sign_model.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2689/2689 - 1615s - 601ms/step - accuracy: 0.9897 - loss: 0.0297 - val_accuracy: 1.0000 - val_loss: 6.5430e-05\n",
            "Epoch 2/5\n",
            "\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "2689/2689 - 1557s - 579ms/step - accuracy: 0.9964 - loss: 0.0146 - val_accuracy: 1.0000 - val_loss: 4.6786e-06\n",
            "Epoch 3/5\n",
            "\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "2689/2689 - 1553s - 577ms/step - accuracy: 1.0000 - loss: 2.4852e-04 - val_accuracy: 1.0000 - val_loss: 1.6543e-07\n",
            "Epoch 4/5\n",
            "\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "2689/2689 - 1535s - 571ms/step - accuracy: 1.0000 - loss: 4.5391e-05 - val_accuracy: 1.0000 - val_loss: 4.7318e-07\n",
            "Epoch 5/5\n",
            "\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "2689/2689 - 1562s - 581ms/step - accuracy: 0.9981 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 2.9719e-07\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Evaluating on the test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0001, Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import json\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/Project_DataSet/fire/WhatsApp Video 2025-06-21 at 01.10.32_b91ff951.mp4\"\n",
        "SEQ_LEN = 15\n",
        "IMG_SIZE = 64\n",
        "MODEL_PATH = \"sign_model.h5\"\n",
        "LABEL_MAP_PATH = \"label_map.json\"\n",
        "\n",
        "\n",
        "model = load_model(MODEL_PATH)\n",
        "\n",
        "with open(LABEL_MAP_PATH, \"r\") as f:\n",
        "    label_map = json.load(f)\n",
        "\n",
        "inv_label_map = {int(k): v for k, v in label_map.items()}\n",
        "\n",
        "\n",
        "def load_video_frames(video_path, seq_len=15, img_size=64):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while len(frames) < seq_len:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (img_size, img_size))\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) < seq_len:\n",
        "        print(f\"Video has only {len(frames)} frames. Padding with last frame.\")\n",
        "        while len(frames) < seq_len:\n",
        "            frames.append(frames[-1])  # pad with last frame\n",
        "\n",
        "    frames = np.array(frames).astype(\"float32\") / 255.0\n",
        "    return np.expand_dims(frames, axis=0)  # shape: (1, seq_len, img_size, img_size, 3)\n",
        "\n",
        "\n",
        "sample_sequence = load_video_frames(VIDEO_PATH, SEQ_LEN, IMG_SIZE)\n",
        "prediction = model.predict(sample_sequence)\n",
        "predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "predicted_label = inv_label_map[predicted_class]\n",
        "\n",
        "print(f\"Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "tnyM9pJWtTWn",
        "outputId": "4b2bf064-b13b-403e-a7f0-04c038b5a09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cv2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-4135080194.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install twilio\n",
        "!pip install open-cv python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn6Aol6LwZZN",
        "outputId": "c02012f3-b506-4ea9-d53e-8187528fb96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: twilio in /usr/local/lib/python3.11/dist-packages (9.6.3)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from twilio) (2.32.3)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /usr/lib/python3/dist-packages (from twilio) (2.3.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.4 in /usr/local/lib/python3.11/dist-packages (from twilio) (3.11.15)\n",
            "Requirement already satisfied: aiohttp-retry>=2.8.3 in /usr/local/lib/python3.11/dist-packages (from twilio) (2.9.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (2025.6.15)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement open-cv (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for open-cv\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Twilio credentials\n",
        "ACCOUNT_SID = \"AC58fa4f42a75cc9f2bdc4bf738d495beb\"\n",
        "AUTH_TOKEN = \"2409004567c530a4355b2254d940938e\"\n",
        "FROM_PHONE = \"+16163445781\"\n",
        "TO_PHONE = \"+917989275641\""
      ],
      "metadata": {
        "id": "rnjizCNv12tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install twilio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcT6gbkZk_O0",
        "outputId": "e90d215d-f325-40c6-a5f6-c7615e2e3ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twilio\n",
            "  Downloading twilio-9.6.3-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from twilio) (2.32.3)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /usr/lib/python3/dist-packages (from twilio) (2.3.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.4 in /usr/local/lib/python3.11/dist-packages (from twilio) (3.11.15)\n",
            "Collecting aiohttp-retry>=2.8.3 (from twilio)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (2025.6.15)\n",
            "Downloading twilio-9.6.3-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: aiohttp-retry, twilio\n",
            "Successfully installed aiohttp-retry-2.9.1 twilio-9.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from twilio.rest import Client\n",
        "\n",
        "def send_sms_alert(message_body):\n",
        "    try:\n",
        "        client = Client(ACCOUNT_SID, AUTH_TOKEN)\n",
        "        message = client.messages.create(\n",
        "            body=message_body,\n",
        "            from_=FROM_PHONE,\n",
        "            to=TO_PHONE\n",
        "        )\n",
        "        print(f\"✅ SMS sent: SID {message.sid}\")    #online copied icons use to look attractive\n",
        "    except Exception as e:\n",
        "        print(f\"❌ SMS failed: {e}\")"
      ],
      "metadata": {
        "id": "XYjoWouOxpzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if predicted_label == \"fire\":\n",
        "    send_sms_alert(\"🔥 Fire Detected! Immediate attention needed.\")\n",
        "elif predicted_label==\"road_accident\":\n",
        "    send_sms_alert(\"Road Accident Detected! Immediate attention needed.\")\n",
        "elif predicted_label==\"voilence\":\n",
        "    send_sms_alert(\"Voilence Detected! Immediate attention needed.\")\n",
        "else:\n",
        "    print(\"No alert needed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "aqu1pH3d11eI",
        "outputId": "e90fbd0e-ff73-4003-dd53-8f3515594b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'predicted_label' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3683922405.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mpredicted_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fire\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msend_sms_alert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🔥 Fire Detected! Immediate attention needed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"road_accident\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msend_sms_alert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Road Accident Detected! Immediate attention needed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"voilence\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predicted_label' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tLRvbX_bfV2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}